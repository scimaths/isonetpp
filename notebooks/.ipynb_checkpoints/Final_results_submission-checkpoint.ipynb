{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/home/ashwinr/btp24/grph/ISONET\n"
     ]
    }
   ],
   "source": [
    "%cd ./../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import random \n",
    "import numpy as np\n",
    "from subgraph.utils import cudavar\n",
    "from GMN.configure import get_default_config\n",
    "from subgraph.earlystopping import EarlyStoppingModule\n",
    "from sklearn.metrics import average_precision_score, ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "av = Namespace(   want_cuda                    = True,\n",
    "                  has_cuda                   = torch.cuda.is_available(),\n",
    "                  use_pairnorm               = False,\n",
    "                  is_sig                     = False,\n",
    "                  n_layers                   = 3,\n",
    "                  conv_type                  = 'SAGE',\n",
    "                  method_type                = 'order',\n",
    "                  skip                       = 'learnable',\n",
    "                  MIN_QUERY_SUBGRAPH_SIZE    = 5,\n",
    "                  MAX_QUERY_SUBGRAPH_SIZE    = 10,\n",
    "                  MIN_CORPUS_SUBGRAPH_SIZE   = 11,\n",
    "                  MAX_CORPUS_SUBGRAPH_SIZE   = 15,\n",
    "                  DIR_PATH                   =\".\",\n",
    "                  DATASET_NAME               = \"ptc_fr\",\n",
    "                  RUN_TILL_ES                = True,\n",
    "                  ES                         = 50,\n",
    "                  transform_dim              = 16,\n",
    "                  GMN_NPROPLAYERS            = 5,\n",
    "                  FEAT_TYPE                  = \"One\",\n",
    "                  filters_1                  = 10,\n",
    "                  filters_2                  = 10,\n",
    "                  filters_3                  = 10,\n",
    "                  neuromatch_hidden_dim      = 10,\n",
    "                  post_mp_dim                = 64,\n",
    "                  bottle_neck_neurons        = 10,\n",
    "                  tensor_neurons             = 10,               \n",
    "                  dropout                    = 0,\n",
    "                  bins                       = 16,\n",
    "                  histogram                  = False,\n",
    "                  WEIGHT_DECAY               =5*10**-4,\n",
    "                  BATCH_SIZE                 =128,\n",
    "                  LEARNING_RATE              =0.001,\n",
    "                  CONV                       = \"GCN\",\n",
    "                  MARGIN                     = 0.1,\n",
    "                  NOISE_FACTOR               = 0,\n",
    "                  NUM_RUNS                   = 2,\n",
    "                  TASK                       = \"\",\n",
    "                  test_size                  = 300,\n",
    "                  SEED                       = 3,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(): \n",
    "  config = get_default_config()\n",
    "\n",
    "  config['encoder'] ['node_hidden_sizes'] = [10]\n",
    "  config['encoder'] ['node_feature_dim'] = 1\n",
    "  config['encoder'] ['edge_feature_dim'] = 1\n",
    "    \n",
    "  config['aggregator'] ['node_hidden_sizes'] = [10]\n",
    "  config['aggregator'] ['graph_transform_sizes'] = [10]\n",
    "  config['aggregator'] ['input_size'] = [10]\n",
    "\n",
    "  config['graph_matching_net'] ['node_state_dim'] = 10\n",
    "  config['graph_matching_net'] ['n_prop_layers'] = av.GMN_NPROPLAYERS\n",
    "  config['graph_matching_net'] ['edge_hidden_sizes'] = [20]\n",
    "  config['graph_matching_net'] ['node_hidden_sizes'] = [10]\n",
    "    \n",
    "  config['graph_embedding_net'] ['node_state_dim'] = 10\n",
    "  config['graph_embedding_net'] ['n_prop_layers'] = av.GMN_NPROPLAYERS\n",
    "  config['graph_embedding_net'] ['edge_hidden_sizes'] = [20]\n",
    "  config['graph_embedding_net'] ['node_hidden_sizes'] = [10]\n",
    "  \n",
    "  config['graphsim']= {}\n",
    "  config['graphsim']['conv_kernel_size'] = [10,4,2]\n",
    "  config['graphsim']['linear_size'] = [24, 16]\n",
    "  config['graphsim']['gcn_size'] = [10,10,10]\n",
    "  config['graphsim']['conv_pool_size'] = [3,3,2]\n",
    "  config['graphsim']['conv_out_channels'] = [2,4,8]\n",
    "  config['graphsim']['dropout'] = av.dropout\n",
    "\n",
    "  config['training']['batch_size']  = av.BATCH_SIZE\n",
    "  config['training']['margin']  = av.MARGIN\n",
    "  config['evaluation']['batch_size']  = av.BATCH_SIZE\n",
    "  config['model_type']  = \"embedding\"\n",
    "    \n",
    "  return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subgraph.iso_matching_models as im\n",
    "\n",
    "es = EarlyStoppingModule(av,50)\n",
    "device = \"cuda\" if av.has_cuda and av.want_cuda else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from grakel.kernels import ShortestPath, RandomWalk\n",
    "# import grakel\n",
    "# def compute_SPKernel(gr):\n",
    "#     grakel_gr = list(grakel.graph_from_networkx(gr))\n",
    "#     gkall = ShortestPath(normalize=True,with_labels=False)\n",
    "#     Kall = gkall.fit_transform(grakel_gr)\n",
    "#     return Kall\n",
    "# def compute_RWKernel(gr):\n",
    "#     grakel_gr = list(grakel.graph_from_networkx(gr))\n",
    "#     gkall = RandomWalk(normalize=True,kernel_type='exponential')\n",
    "#     Kall = gkall.fit_transform(grakel_gr)\n",
    "#     return Kall\n",
    "\n",
    "\n",
    "def print_metric_table(all_results, id2, model_list = [\"GMN-embed\", \"GMN-colbert\", \"GMN-colbert-mlp\", \"NeuroMatch\", \"ISONET\", \"Node-align(Node loss)\", \"Node-align(Edge loss)\", \"GMN-embed-Asym\", \"ISONET-Sym\"]):\n",
    "    id1 = 1 #val=0, test=1\n",
    "    print(\"\\\\begin{table}[hbt!]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\begin{tabular}{l|c|c|c|c|c|c} \")\n",
    "    print(\"\\\\hline \")\n",
    "\n",
    "    print(\" & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\\\\\hline\")\n",
    "    for s in model_list:\n",
    "        res = all_results[s]\n",
    "        for dataset in ['ptc_fr', 'ptc_fm', 'ptc_mm', 'ptc_mr', 'mutag', 'aids']:\n",
    "            try:\n",
    "                s = s + \" & \" + \"{:.2f} $\\pm$ {:.2f}\".format(res[dataset][id1][id2], res[dataset][id1][id2+1]/np.sqrt(av.test_size))\n",
    "            except:\n",
    "                pass\n",
    "        print(s,\"\\\\\\\\\")\n",
    "\n",
    "    print(\"\\\\end{tabular} \")\n",
    "    print(\"\\\\end{table}\")\n",
    "\n",
    "def evaluate_embeddings_similarity_map_mrr_mndcg(av,model,sampler):\n",
    "  model.eval()\n",
    "  d_pos = sampler.list_pos\n",
    "  d_neg = sampler.list_neg\n",
    "\n",
    "  d = d_pos + d_neg\n",
    "  npos = len(d_pos)\n",
    "  nneg = len(d_neg)\n",
    "\n",
    "  pred = []\n",
    "\n",
    "  n_batches = sampler.create_batches(d)\n",
    "  for i in range(n_batches):\n",
    "    #ignoring target values here since not needed for AP ranking score \n",
    "    batch_data,batch_data_sizes,_,batch_adj = sampler.fetch_batched_data_by_id(i)\n",
    "    pred.append( model(batch_data,batch_data_sizes,batch_adj).data)\n",
    "\n",
    "  all_pred = torch.cat(pred,dim=0) \n",
    "  labels = cudavar(av,torch.cat((torch.ones(npos),torch.zeros(nneg))))\n",
    "  ap_score   = average_precision_score(labels.cpu(), all_pred.cpu())\n",
    "  so = np.argsort(all_pred.cpu()).tolist()[::-1]\n",
    "  labels_rearranged = labels.cpu()[so]\n",
    "  rr = 1/(labels_rearranged.tolist().index(1)+1)\n",
    "  ndcg = ndcg_score([labels.cpu().tolist()],[all_pred.cpu().tolist()])\n",
    "\n",
    "  q_graphs = list(range(len(sampler.query_graphs)))   \n",
    "    \n",
    "  all_ap, all_rr, all_ndcg = [], [], []\n",
    "\n",
    "  for q_id in q_graphs:\n",
    "    dpos = list(filter(lambda x:x[0][0]==q_id,d_pos))\n",
    "    dneg = list(filter(lambda x:x[0][0]==q_id,d_neg))\n",
    "    npos = len(dpos)\n",
    "    nneg = len(dneg)\n",
    "    d = dpos+dneg\n",
    "    if npos>0 and nneg>0:    \n",
    "      #Damn\n",
    "      n_batches = sampler.create_batches(d) \n",
    "      pred = []  \n",
    "      for i in range(n_batches):\n",
    "        #ignoring known ged values here since not needed for AP ranking score \n",
    "        batch_data,batch_data_sizes,_,batch_adj = sampler.fetch_batched_data_by_id(i)\n",
    "        pred.append( model(batch_data,batch_data_sizes,batch_adj).data)\n",
    "      all_pred = torch.cat(pred,dim=0) \n",
    "      labels = cudavar(av,torch.cat((torch.ones(npos),torch.zeros(nneg))))\n",
    "      ap   = average_precision_score(labels.cpu(), all_pred.cpu()) \n",
    "      all_ap.append(ap)\n",
    "      so = np.argsort(all_pred.cpu()).tolist()[::-1]\n",
    "      labels_rearranged = labels.cpu()[so]\n",
    "      all_rr.append(1/(labels_rearranged.tolist().index(1)+1))\n",
    "      all_ndcg.append(ndcg_score([labels.cpu().tolist()],[all_pred.cpu().tolist()]))\n",
    "  return ap_score, np.mean(all_ap), np.std(all_ap), rr, np.mean(all_rr), np.std(all_rr), ndcg, np.mean(all_ndcg), np.std(all_ndcg), all_ap, all_rr\n",
    "\n",
    "\n",
    "def evaluate_embeddings_similarity_kernels(av,sim_matrix,sampler):\n",
    "  #model.eval()\n",
    "    d_pos = sampler.list_pos\n",
    "    d_neg = sampler.list_neg\n",
    "    \n",
    "    d = d_pos + d_neg\n",
    "    npos = len(d_pos)\n",
    "    nneg = len(d_neg)\n",
    "\n",
    "    pred = []\n",
    "    \n",
    "    for i in range(len(d_pos)) :\n",
    "        q = d_pos[i][0][0]\n",
    "        c = d_pos[i][0][1]\n",
    "        pred.append(sim_matrix[q, c+len(sampler.query_graphs)])\n",
    "    for i in range(len(d_neg)) :\n",
    "        q = d_neg[i][0][0]\n",
    "        c = d_neg[i][0][1]\n",
    "        pred.append(sim_matrix[q, c+len(sampler.query_graphs)]) \n",
    "\n",
    "    all_pred = np.array(pred) \n",
    "    labels = np.concatenate((np.ones(npos),np.zeros(nneg)))\n",
    "    ap_score   = average_precision_score(labels, all_pred)\n",
    "    so = np.argsort(all_pred).tolist()[::-1]\n",
    "    labels_rearranged = labels[so]\n",
    "    rr = 1/(labels_rearranged.tolist().index(1)+1)\n",
    "    ndcg = ndcg_score([labels],[all_pred])\n",
    "\n",
    "    q_graphs = list(range(len(sampler.query_graphs)))    \n",
    "    \n",
    "    all_ap = []\n",
    "    all_ndcg = []\n",
    "    all_rr = []\n",
    "\n",
    "    for q_id in q_graphs:\n",
    "        dpos = list(filter(lambda x:x[0][0]==q_id,d_pos))\n",
    "        dneg = list(filter(lambda x:x[0][0]==q_id,d_neg))\n",
    "        npos = len(dpos)\n",
    "        nneg = len(dneg)\n",
    "        d = dpos+dneg\n",
    "        if npos>0 and nneg>0:     \n",
    "            pred = []  \n",
    "            for i in range(len(dpos)) :\n",
    "                q = dpos[i][0][0]\n",
    "                c = dpos[i][0][1]\n",
    "                pred.append(sim_matrix[q, c+len(sampler.query_graphs)])\n",
    "            for i in range(len(dneg)) :\n",
    "                q = dneg[i][0][0]\n",
    "                c = dneg[i][0][1]\n",
    "                pred.append(sim_matrix[q, c+len(sampler.query_graphs)]) \n",
    "\n",
    "            all_pred = np.array(pred) \n",
    "            labels = np.concatenate((np.ones(npos),np.zeros(nneg)))\n",
    "            all_ap.append(average_precision_score(labels, all_pred))\n",
    "            so = np.argsort(all_pred).tolist()[::-1]\n",
    "            labels_rearranged = labels[so]\n",
    "            all_rr.append(1/(labels_rearranged.tolist().index(1)+1))\n",
    "            all_ndcg.append(ndcg_score([labels],[all_pred]))\n",
    "\n",
    "    return ap_score, np.mean(all_ap), np.std(all_ap), rr, np.mean(all_rr), np.std(all_rr), ndcg, np.mean(all_ndcg), np.std(all_ndcg), all_ap, all_rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_gmn_data():\n",
    "    data_mode = \"test\" if av.test_size==25 else \"Extra_test_300\"\n",
    "    test_data = im.OurMatchingModelSubgraphIsoData(av,mode=data_mode)\n",
    "    val_data = im.OurMatchingModelSubgraphIsoData(av,mode=\"val\")\n",
    "    test_data.data_type = \"pyg\"\n",
    "    val_data.data_type = \"pyg\"\n",
    "    return val_data, test_data\n",
    "\n",
    "def get_result(task,dataset):\n",
    "    #config = load_config()\n",
    "    av.TASK = task\n",
    "    av.DATASET_NAME = dataset \n",
    "\n",
    "    if av.FEAT_TYPE == \"Adjrow\" or  av.FEAT_TYPE == \"Adjrow1\" or av.FEAT_TYPE == \"AdjOnehot\": \n",
    "      av.TASK = av.TASK + \"_\" + av.FEAT_TYPE\n",
    "\n",
    "    val_data, test_data = fetch_gmn_data()\n",
    "    if av.TASK.startswith(\"matching_iso_var_18_gmn_sinkhorn_param_big_hinge_score_on_embeds\"):\n",
    "      config = load_config()\n",
    "      model = im.Node_align_Node_loss(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_19_gmn_all\"):\n",
    "      config = load_config()\n",
    "      model = im.GMN_embed(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_gmn_with_colbert\"):\n",
    "      config = load_config()\n",
    "      model = im.GMN_embed_with_ColBERT_scores(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_gmn_with_mlp_and_colbert_objective\"):\n",
    "      config = load_config()\n",
    "      model = im.GMN_embed_with_MLP_and_ColBERT_scores(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_27_gmn_edge_perm_sinkhorn_param_big_hinge_score_on_edges\"):\n",
    "      #One more hack. \n",
    "      av.MAX_EDGES = max(max([g.number_of_edges() for g in test_data.query_graphs]),\\\n",
    "                   max([g.number_of_edges() for g in test_data.corpus_graphs]))\n",
    "      config = load_config()\n",
    "      model = im.ISONET(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_29_gmn_sinkhorn_param_big_hinge_score_on_edge_similarity_ff_adj_mask\"):\n",
    "      config = load_config()\n",
    "      model = im.Node_align_Edge_loss(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_34_gmn_embed_hinge\"):\n",
    "      config = load_config()\n",
    "      model = im.GMN_embed_hinge(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_35_gmn_match_hinge\"):\n",
    "      config = load_config()\n",
    "      model = im.GMN_match_hinge(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_36_gmn_edge_perm_sinkhorn_param_big_sqeuc_score_on_edges\"):\n",
    "      av.MAX_EDGES = max(max([g.number_of_edges() for g in test_data.query_graphs]),\\\n",
    "                     max([g.number_of_edges() for g in test_data.corpus_graphs]))\n",
    "      config = load_config()\n",
    "      model = im.ISONET_Sym(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"simgnn_noperm\") :\n",
    "      config = load_config()\n",
    "      model = im.SimGNN(av,1).to(device)\n",
    "      test_data.data_type = \"pyg\"\n",
    "      val_data.data_type = \"pyg\"    \n",
    "    elif av.TASK.startswith(\"gmn_match\"):\n",
    "      config = load_config()\n",
    "      model = im.GMN_match(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    elif av.TASK.startswith(\"matching_iso_graphsim\"):\n",
    "      config = load_config()\n",
    "      model = im.GraphSim(av,config,1).to(device)\n",
    "      test_data.data_type = \"pyg\"\n",
    "      val_data.data_type = \"pyg\"\n",
    "    elif av.TASK.startswith(\"matching_iso_neuromatch\"):\n",
    "      config = load_config()\n",
    "      model = im.NeuroMatch(1,av.neuromatch_hidden_dim,av).to(device)\n",
    "      test_data.data_type = \"pyg\"\n",
    "      val_data.data_type = \"pyg\"\n",
    "    elif av.TASK.startswith(\"ir_modified_gotsim\"):\n",
    "      config = load_config()\n",
    "      model = im.GOTSim(av,config,1).to(device)\n",
    "      test_data.data_type = \"pyg\"\n",
    "      val_data.data_type = \"pyg\"\n",
    "    elif av.TASK.startswith(\"matching_iso_var_gmn_with_maxsim_dot_corrected\"):\n",
    "      config = load_config()\n",
    "      model = im.GMN_embed_maxsim_dot_corrected(av,config,1).to(device)\n",
    "      test_data.data_type = \"gmn\"\n",
    "      val_data.data_type = \"gmn\"\n",
    "    else:\n",
    "      print(\"ALERT!! CHECK FOR ERROR\")  \n",
    "    model.eval()\n",
    "    checkpoint = es.load_best_model()\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    val_result = evaluate_embeddings_similarity_map_mrr_mndcg(av,model,val_data)\n",
    "    test_result = evaluate_embeddings_similarity_map_mrr_mndcg(av,model,test_data)\n",
    "\n",
    "    return val_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_dict = {} \n",
    "\n",
    "task_dict['Node-align(Node loss)'] = {}\n",
    "task_dict['Node-align(Node loss)'][\"ptc_fm\"] = \"matching_iso_var_18_gmn_sinkhorn_param_big_hinge_score_on_embeds_run7_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Node loss)'][\"ptc_fr\"] = \"matching_iso_var_18_gmn_sinkhorn_param_big_hinge_score_on_embeds_run7_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Node loss)'][\"ptc_mr\"] = \"matching_iso_var_18_gmn_sinkhorn_param_big_hinge_score_on_embeds_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Node loss)'][\"ptc_mm\"] = \"matching_iso_var_18_gmn_sinkhorn_param_big_hinge_score_on_embeds_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Node loss)'][\"mutag\"]  = \"matching_iso_var_18_gmn_sinkhorn_param_big_hinge_score_on_embeds_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Node loss)'][\"aids\"]   = \"matching_iso_var_18_gmn_sinkhorn_param_big_hinge_score_on_embeds_run1_corrected_noise0_margin2E-1\"\n",
    "\n",
    "task_dict['GMN-embed'] = {}\n",
    "task_dict['GMN-embed'][\"ptc_fm\"] = \"matching_iso_var_19_gmn_all_run3_margin2E-1_corrected\"\n",
    "task_dict['GMN-embed'][\"ptc_fr\"] = \"matching_iso_var_19_gmn_all_run3_margin2E-1_corrected\"\n",
    "task_dict['GMN-embed'][\"ptc_mr\"] = \"matching_iso_var_19_gmn_all_run1_margin2E-1_corrected\"\n",
    "task_dict['GMN-embed'][\"ptc_mm\"] = \"matching_iso_var_19_gmn_all_run1_margin2E-1_corrected\"\n",
    "task_dict['GMN-embed'][\"mutag\"]  = \"matching_iso_var_19_gmn_all_run1_margin2E-1_corrected\"\n",
    "task_dict['GMN-embed'][\"aids\"]   = \"matching_iso_var_19_gmn_all_run1_margin2E-1_corrected\"\n",
    "\n",
    "task_dict['GMN-colbert'] = {}\n",
    "task_dict['GMN-colbert'][\"ptc_fr\"] = \"matching_iso_var_gmn_with_colbert_objective\"\n",
    "task_dict['GMN-colbert'][\"ptc_fm\"] = \"matching_iso_var_gmn_with_colbert_objective\"\n",
    "task_dict['GMN-colbert'][\"ptc_mr\"] = \"matching_iso_var_gmn_with_colbert_objective\"\n",
    "task_dict['GMN-colbert'][\"ptc_mm\"] = \"matching_iso_var_gmn_with_colbert_objective\"\n",
    "task_dict['GMN-colbert'][\"mutag\"] = \"matching_iso_var_gmn_with_colbert_objective\"\n",
    "task_dict['GMN-colbert'][\"aids\"] = \"matching_iso_var_gmn_with_colbert_objective\"\n",
    "\n",
    "\n",
    "task_dict['GMN-colbert-mlp'] = {}\n",
    "task_dict['GMN-colbert-mlp'][\"ptc_fr\"] = \"matching_iso_var_gmn_with_mlp_and_colbert_objective\"\n",
    "task_dict['GMN-colbert-mlp'][\"ptc_fm\"] = \"matching_iso_var_gmn_with_mlp_and_colbert_objective\"\n",
    "task_dict['GMN-colbert-mlp'][\"ptc_mr\"] = \"matching_iso_var_gmn_with_mlp_and_colbert_objective\"\n",
    "task_dict['GMN-colbert-mlp'][\"ptc_mm\"] = \"matching_iso_var_gmn_with_mlp_and_colbert_objective\"\n",
    "task_dict['GMN-colbert-mlp'][\"mutag\"] = \"matching_iso_var_gmn_with_mlp_and_colbert_objective\"\n",
    "task_dict['GMN-colbert-mlp'][\"aids\"] = \"matching_iso_var_gmn_with_mlp_and_colbert_objective\"\n",
    "\n",
    "task_dict['GMN-colbert-maxsim-dot-corrected'] = {}\n",
    "task_dict['GMN-colbert-maxsim-dot-corrected'][\"ptc_fr\"] = \"matching_iso_var_gmn_with_maxsim_dot_corrected\"\n",
    "task_dict['GMN-colbert-maxsim-dot-corrected'][\"ptc_fm\"] = \"matching_iso_var_gmn_with_maxsim_dot_corrected\"\n",
    "task_dict['GMN-colbert-maxsim-dot-corrected'][\"ptc_mr\"] = \"matching_iso_var_gmn_with_maxsim_dot_corrected\"\n",
    "task_dict['GMN-colbert-maxsim-dot-corrected'][\"ptc_mm\"] = \"matching_iso_var_gmn_with_maxsim_dot_corrected\"\n",
    "task_dict['GMN-colbert-maxsim-dot-corrected'][\"mutag\"] = \"matching_iso_var_gmn_with_maxsim_dot_corrected\"\n",
    "task_dict['GMN-colbert-maxsim-dot-corrected'][\"aids\"] = \"matching_iso_var_gmn_with_maxsim_dot_corrected\"\n",
    "\n",
    "task_dict['ISONET'] = {}\n",
    "task_dict['ISONET'][\"ptc_fm\"] = \"matching_iso_var_27_gmn_edge_perm_sinkhorn_param_big_hinge_score_on_edges_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['ISONET'][\"ptc_fr\"] = \"matching_iso_var_27_gmn_edge_perm_sinkhorn_param_big_hinge_score_on_edges_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['ISONET'][\"ptc_mr\"] = \"matching_iso_var_27_gmn_edge_perm_sinkhorn_param_big_hinge_score_on_edges_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['ISONET'][\"ptc_mm\"] = \"matching_iso_var_27_gmn_edge_perm_sinkhorn_param_big_hinge_score_on_edges_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['ISONET'][\"mutag\"]  = \"matching_iso_var_27_gmn_edge_perm_sinkhorn_param_big_hinge_score_on_edges_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['ISONET'][\"aids\"]   = \"matching_iso_var_27_gmn_edge_perm_sinkhorn_param_big_hinge_score_on_edges_run1_corrected_noise0_margin2E-1\"\n",
    "\n",
    "task_dict['Node-align(Edge loss)'] = {}\n",
    "task_dict['Node-align(Edge loss)'][\"ptc_fm\"] = \"matching_iso_var_29_gmn_sinkhorn_param_big_hinge_score_on_edge_similarity_ff_adj_mask_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Edge loss)'][\"ptc_fr\"] = \"matching_iso_var_29_gmn_sinkhorn_param_big_hinge_score_on_edge_similarity_ff_adj_mask_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Edge loss)'][\"ptc_mr\"] = \"matching_iso_var_29_gmn_sinkhorn_param_big_hinge_score_on_edge_similarity_ff_adj_mask_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Edge loss)'][\"ptc_mm\"] = \"matching_iso_var_29_gmn_sinkhorn_param_big_hinge_score_on_edge_similarity_ff_adj_mask_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Edge loss)'][\"mutag\"]  = \"matching_iso_var_29_gmn_sinkhorn_param_big_hinge_score_on_edge_similarity_ff_adj_mask_run1_corrected_noise0_margin2E-1\"\n",
    "task_dict['Node-align(Edge loss)'][\"aids\"]   = \"matching_iso_var_29_gmn_sinkhorn_param_big_hinge_score_on_edge_similarity_ff_adj_mask_run1_corrected_noise0_margin2E-1\"\n",
    "\n",
    "task_dict['GraphSim'] = {}\n",
    "task_dict['GraphSim'][\"ptc_fm\"] = \"matching_iso_graphsim_score_logits_pair_loss_NoInterpolation_NoBfs_One_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['GraphSim'][\"ptc_fr\"] = \"matching_iso_graphsim_score_logits_pair_loss_NoInterpolation_NoBfs_One_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['GraphSim'][\"ptc_mr\"] = \"matching_iso_graphsim_score_logits_pair_loss_NoInterpolation_NoBfs_One_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['GraphSim'][\"ptc_mm\"] = \"matching_iso_graphsim_score_logits_pair_loss_NoInterpolation_NoBfs_One_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['GraphSim'][\"mutag\"]  = \"matching_iso_graphsim_score_logits_pair_loss_NoInterpolation_NoBfs_One_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['GraphSim'][\"aids\"]   = \"matching_iso_graphsim_score_logits_pair_loss_NoInterpolation_NoBfs_One_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "\n",
    "    \n",
    "task_dict['GOTSim'] = {}\n",
    "for dataset in ['ptc_fr','ptc_fm','ptc_mr','ptc_mm','mutag','aids' ]:\n",
    "  task_dict['GOTSim'][dataset] = \"ir_modified_gotsim_run1\"\n",
    "\n",
    "\n",
    "task_dict['GMN-embed-Asym'] = {}\n",
    "task_dict['GMN-match-Asym'] = {}\n",
    "task_dict['ISONET-Sym'] = {}\n",
    "for dataset in ['ptc_fr','ptc_fm','ptc_mr','ptc_mm','mutag','aids' ]:\n",
    "  task_dict['GMN-embed-Asym'][dataset] = \"matching_iso_var_34_gmn_embed_hinge_run1_margin2E-1\"\n",
    "  if dataset == \"ptc_fm\":\n",
    "    task_dict['GMN-match-Asym'][dataset] = \"matching_iso_var_35_gmn_match_hinge_run2_margin2E-1\"\n",
    "  else:\n",
    "    task_dict['GMN-match-Asym'][dataset] = \"matching_iso_var_35_gmn_match_hinge_run1_margin2E-1\"\n",
    "  task_dict['ISONET-Sym'][dataset] = \"matching_iso_var_36_gmn_edge_perm_sinkhorn_param_big_sqeuc_score_on_edges_run1_corrected_noise0_margin2E-1\"\n",
    "\n",
    "\n",
    "task_dict['NeuroMatch'] = {}\n",
    "task_dict['NeuroMatch'][\"ptc_fm\"] = \"matching_iso_neuromatch_hid_dim10_post_mp64_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['NeuroMatch'][\"ptc_fr\"] = \"matching_iso_neuromatch_hid_dim10_post_mp64_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['NeuroMatch'][\"ptc_mr\"] = \"matching_iso_neuromatch_hid_dim10_post_mp64_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['NeuroMatch'][\"ptc_mm\"] = \"matching_iso_neuromatch_hid_dim10_post_mp64_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['NeuroMatch'][\"mutag\"]  = \"matching_iso_neuromatch_hid_dim10_post_mp64_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "task_dict['NeuroMatch'][\"aids\"]   = \"matching_iso_neuromatch_hid_dim10_post_mp64_dropout0_margin5E-1_run1_corrected_noise0_margin5E-1\"\n",
    "\n",
    "\n",
    "task_dict['SimGNN'] = {}\n",
    "task_dict['SimGNN'][\"ptc_fm\"] = \"simgnn_noperm_run4_margin2E-1\"\n",
    "task_dict['SimGNN'][\"ptc_fr\"] = \"simgnn_noperm_run3_margin10E-1\"\n",
    "task_dict['SimGNN'][\"ptc_mr\"] = \"simgnn_noperm_run4_margin2E-1\" \n",
    "task_dict['SimGNN'][\"ptc_mm\"] = \"simgnn_noperm_run4_margin2E-1\" \n",
    "task_dict['SimGNN'][\"mutag\"]  = \"simgnn_noperm_run3_margin10E-1\" \n",
    "task_dict['SimGNN'][\"aids\"]   = \"simgnn_noperm_run4_margin2E-1\" \n",
    "\n",
    "task_dict['GMN-match'] = {}\n",
    "task_dict['GMN-match'][\"ptc_fm\"] = \"gmn_match_run1_margin2E-1\"\n",
    "task_dict['GMN-match'][\"ptc_fr\"] = \"gmn_match_run1_margin2E-1\"\n",
    "task_dict['GMN-match'][\"ptc_mr\"] = \"gmn_match_run1_margin2E-1\" \n",
    "task_dict['GMN-match'][\"ptc_mm\"] = \"gmn_match_run1_margin2E-1\" \n",
    "task_dict['GMN-match'][\"mutag\"]  = \"gmn_match_run2_margin2E-1\" \n",
    "task_dict['GMN-match'][\"aids\"]   = \"gmn_match_run1_margin2E-1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results table for 25 query graphs\n",
    "\n",
    "# Takes 30+ min without Final_results_for_25_query_graphs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test query graphs from ./Datasets/splits/test/test_ptc_fr80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_fr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fr80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_fr80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_fr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fr80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_mlp_and_colbert_objective_ptc_fr_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_ptc_fm80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_fm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fm80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_fm80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_fm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fm80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_mlp_and_colbert_objective_ptc_fm_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_ptc_mm80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_mm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mm80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_mm80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_mm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mm80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_mlp_and_colbert_objective_ptc_mm_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_ptc_mr80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_mr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mr80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_mr80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_mr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mr80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_mlp_and_colbert_objective_ptc_mr_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_mutag80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_mutag80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/mutag80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_mutag80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_mutag80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/mutag80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_mlp_and_colbert_objective_mutag_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_aids80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_aids80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/aids80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_aids80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_aids80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/aids80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_mlp_and_colbert_objective_aids_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_ptc_fr80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_fr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fr80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_fr80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_fr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fr80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_maxsim_dot_corrected_ptc_fr_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_ptc_fm80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_fm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fm80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_fm80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_fm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_fm80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_maxsim_dot_corrected_ptc_fm_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_ptc_mm80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_mm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mm80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_mm80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_mm80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mm80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_maxsim_dot_corrected_ptc_mm_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_ptc_mr80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_ptc_mr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mr80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_ptc_mr80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_ptc_mr80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/ptc_mr80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_maxsim_dot_corrected_ptc_mr_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_mutag80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_mutag80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/mutag80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_mutag80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_mutag80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/mutag80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_maxsim_dot_corrected_mutag_3\n",
      "loading test query graphs from ./Datasets/splits/test/test_aids80k_query_subgraphs.pkl\n",
      "loading test relationships from ./Datasets/splits/test/test_aids80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/aids80k_corpus_subgraphs.pkl\n",
      "loading val query graphs from ./Datasets/splits/val/val_aids80k_query_subgraphs.pkl\n",
      "loading val relationships from ./Datasets/splits/val/val_aids80k_rel_nx_is_subgraph_iso.pkl\n",
      "loading corpus graphs from ./Datasets/splits/aids80k_corpus_subgraphs.pkl\n",
      "loading best validated model from ./bestValidationModels/matching_iso_var_gmn_with_maxsim_dot_corrected_aids_3\n"
     ]
    }
   ],
   "source": [
    "fp_25 = av.DIR_PATH +\"/Datasets/\" +\"Final_results_for_25_query_graphs\"+\".pkl\"\n",
    "av.test_size=25\n",
    "\n",
    "# if os.path.isfile(fp_25):\n",
    "if False:\n",
    "    with open(fp_25, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "else:\n",
    "    all_results = {}\n",
    "#     for task in [\"GMN-embed\", \"GMN-colbert\", \"GMN-colbert-mlp\", \\\n",
    "#                  \"NeuroMatch\", \"ISONET\", \"Node-align(Node loss)\", \"Node-align(Edge loss)\", \\\n",
    "#                  \"GMN-embed-Asym\", \"ISONET-Sym\"]:\n",
    "    for task in [\"GMN-colbert-mlp\", \"GMN-colbert-maxsim-dot-corrected\"]:\n",
    "    \n",
    "        all_results[task] = {}\n",
    "        if task in [\"GraphSim\", \"NeuroMatch\"]:\n",
    "            av.MARGIN = 0.5\n",
    "        else:\n",
    "            av.MARGIN = 0.1\n",
    "    \n",
    "        for dataset in [\"ptc_fr\", \"ptc_fm\", \"ptc_mm\", \"ptc_mr\", \"mutag\", \"aids\"]:\n",
    "#         for dataset in [ \"ptc_fr\"]:\n",
    "            if dataset in task_dict[task].keys():\n",
    "                all_results[task][dataset] =  get_result(task_dict[task][dataset],dataset)\n",
    "            else:\n",
    "                all_results[task][dataset] = (0, 0)\n",
    "\n",
    "    with open(fp_25, 'wb') as f:\n",
    "        pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ptc_fr', 'ptc_fm', 'ptc_mm', 'ptc_mr', 'mutag', 'aids'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['GMN-colbert-maxsim-dot-corrected'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This reproduces Table2, 3 and 4 in main paper and Table10, 11 in Appendix (run above cell before running this cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[hbt!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c|c|c|c|c|c} \n",
      "\\hline \n",
      " & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\hline\n",
      "GMN-colbert-mlp & 0.76 $\\pm$ 0.04 & 0.77 $\\pm$ 0.04 & 0.82 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 & 0.84 $\\pm$ 0.05 & 0.81 $\\pm$ 0.04 \\\\\n",
      "GMN-colbert-maxsim-dot-corrected & 0.75 $\\pm$ 0.05 & 0.77 $\\pm$ 0.05 & 0.80 $\\pm$ 0.04 & 0.77 $\\pm$ 0.04 & 0.87 $\\pm$ 0.04 & 0.81 $\\pm$ 0.04 \\\\\n",
      "\\end{tabular} \n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "id2 = 1 #metric_id_map = {'MAP':1, 'MRR':4, 'MNDCG':7}\n",
    "print_metric_table(all_results, id2, model_list=[\"GMN-colbert-mlp\", \"GMN-colbert-maxsim-dot-corrected\"])\n",
    "# print_metric_table(all_results, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[hbt!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c|c|c|c|c|c} \n",
      "\\hline \n",
      " & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\hline\n",
      "GMN-colbert-mlp & 0.77 $\\pm$ 0.04 & 0.80 $\\pm$ 0.05 & 0.82 $\\pm$ 0.04 & 0.81 $\\pm$ 0.04 & 0.79 $\\pm$ 0.05 & 0.84 $\\pm$ 0.04 \\\\\n",
      "GMN-colbert-maxsim-dot-corrected & 0.77 $\\pm$ 0.05 & 0.77 $\\pm$ 0.04 & 0.81 $\\pm$ 0.04 & 0.78 $\\pm$ 0.04 & 0.81 $\\pm$ 0.05 & 0.80 $\\pm$ 0.04 \\\\\n",
      "\\end{tabular} \n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "id2 = 1 #metric_id_map = {'MAP':1, 'MRR':4, 'MNDCG':7}\n",
    "print_metric_table(all_results, id2, model_list=[\"GMN-colbert-mlp\", \"GMN-colbert-maxsim-dot-corrected\"])\n",
    "# print_metric_table(all_results, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[hbt!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c|c|c|c|c|c} \n",
      "\\hline \n",
      " & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\hline\n",
      "GMN-colbert-mlp & 0.77 $\\pm$ 0.04 & 0.78 $\\pm$ 0.04 & 0.82 $\\pm$ 0.04 & 0.78 $\\pm$ 0.04 & 0.84 $\\pm$ 0.05 & 0.77 $\\pm$ 0.04 \\\\\n",
      "GMN-colbert-maxsim-dot-corrected & 0.80 $\\pm$ 0.04 & 0.76 $\\pm$ 0.04 & 0.79 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 & 0.79 $\\pm$ 0.05 & 0.79 $\\pm$ 0.04 \\\\\n",
      "\\end{tabular} \n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "id2 = 1 #metric_id_map = {'MAP':1, 'MRR':4, 'MNDCG':7}\n",
    "print_metric_table(all_results, id2, model_list=[\"GMN-colbert-mlp\", \"GMN-colbert-maxsim-dot-corrected\"])\n",
    "# print_metric_table(all_results, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[hbt!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c|c|c|c|c|c} \n",
      "\\hline \n",
      " & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\hline\n",
      "GMN-colbert-mlp & 0.76 $\\pm$ 0.05 & 0.76 $\\pm$ 0.04 & 0.83 $\\pm$ 0.04 & 0.77 $\\pm$ 0.04 & 0.85 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 \\\\\n",
      "GMN-colbert-maxsim-dot-corrected & 0.75 $\\pm$ 0.04 & 0.79 $\\pm$ 0.05 & 0.82 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 & 0.86 $\\pm$ 0.04 & 0.82 $\\pm$ 0.05 \\\\\n",
      "\\end{tabular} \n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "id2 = 1 #metric_id_map = {'MAP':1, 'MRR':4, 'MNDCG':7}\n",
    "print_metric_table(all_results, id2, model_list=[\"GMN-colbert-mlp\", \"GMN-colbert-maxsim-dot-corrected\"])\n",
    "# print_metric_table(all_results, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seed 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[hbt!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c|c|c|c|c|c} \n",
      "\\hline \n",
      " & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\hline\n",
      "GMN-colbert-mlp & 0.79 $\\pm$ 0.05 & 0.82 $\\pm$ 0.04 & 0.78 $\\pm$ 0.04 & 0.79 $\\pm$ 0.06 & 0.84 $\\pm$ 0.04 \\\\\n",
      "GMN-colbert-maxsim-dot-corrected & 0.73 $\\pm$ 0.05 & 0.78 $\\pm$ 0.04 & 0.78 $\\pm$ 0.04 & 0.78 $\\pm$ 0.06 & 0.82 $\\pm$ 0.04 \\\\\n",
      "\\end{tabular} \n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "id2 = 1 #metric_id_map = {'MAP':1, 'MRR':4, 'MNDCG':7}\n",
    "print_metric_table(all_results, id2, model_list=[\"GMN-colbert-mlp\", \"GMN-colbert-maxsim-dot-corrected\"])\n",
    "# print_metric_table(all_results, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results table for 300 query graphs\n",
    "\n",
    "# Takes 2hr+ without Final_results_for_300_query_graphs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_300 = av.DIR_PATH +\"/Datasets/\" +\"Final_results_for_300_query_graphs\"+\".pkl\"\n",
    "av.test_size=300\n",
    "\n",
    "if os.path.isfile(fp_300):\n",
    "    with open(fp_300, 'rb') as f:\n",
    "        all_results = pickle.load(f)\n",
    "else:\n",
    "    all_results = {}\n",
    "    for task in [\"SPKernel\", \"RWKernel\", \"GraphSim\", \"GOTSim\", \"SimGNN\", \"GMN-embed\", \"GMN-match\", \\\n",
    "                 \"NeuroMatch\", \"ISONET\", \"Node-align(Node loss)\", \"Node-align(Edge loss)\", \\\n",
    "                 \"GMN-embed-Asym\", \"GMN-match-Asym\", \"ISONET-Sym\"]:\n",
    "    \n",
    "        all_results[task] = {}\n",
    "        if task in [\"GraphSim\", \"NeuroMatch\"]:\n",
    "            av.MARGIN = 0.5\n",
    "        else:\n",
    "            av.MARGIN = 0.1\n",
    "    \n",
    "        if task == \"SPKernel\":\n",
    "\n",
    "            for dataset in ['ptc_fr', 'ptc_fm', 'ptc_mr', 'ptc_mm', 'aids', 'mutag']:  \n",
    "    \n",
    "                av.TASK = task\n",
    "                av.DATASET_NAME = dataset \n",
    "            \n",
    "                data_mode = \"test\" if av.test_size==25 else \"Extra_test_300\"\n",
    "                sampler = im.OurMatchingModelSubgraphIsoData(av,mode=data_mode)\n",
    "                sampler.data_type = \"gmn\"\n",
    "            \n",
    "                fp = av.DIR_PATH+\"/Datasets/\" +\"SPKernel_iso_\"+str(av.test_size)+\"_\"+ av.DATASET_NAME\n",
    "                if os.path.isfile(fp):\n",
    "                    with open(fp, 'rb') as f:\n",
    "                        SPK = pickle.load(f)\n",
    "                else:\n",
    "                    SPK = compute_SPKernel(sampler.query_graphs + sampler.corpus_graphs )\n",
    "                    with open(fp, 'wb') as f:\n",
    "                        pickle.dump(SPK, f)\n",
    "    \n",
    "            \n",
    "                all_results[task][dataset] = (_, evaluate_embeddings_similarity_kernels(av, SPK, sampler))\n",
    "            \n",
    "        elif task == \"RWKernel\":\n",
    "            for dataset in ['ptc_fr', 'ptc_fm', 'ptc_mr', 'ptc_mm', 'aids', 'mutag']:  \n",
    "    \n",
    "                av.TASK = task\n",
    "                av.DATASET_NAME = dataset \n",
    "            \n",
    "                data_mode = \"test\" if av.test_size==25 else \"Extra_test_300\"\n",
    "                sampler = im.OurMatchingModelSubgraphIsoData(av,mode=data_mode)\n",
    "                sampler.data_type = \"gmn\"\n",
    "            \n",
    "                fp = av.DIR_PATH+\"/Datasets/\" +\"RWKernel_iso_\"+str(av.test_size)+\"_\"+ av.DATASET_NAME\n",
    "                if os.path.isfile(fp):\n",
    "                    with open(fp, 'rb') as f:\n",
    "                        RWK = pickle.load(f)\n",
    "                else:\n",
    "                    RWK = compute_RWKernel(sampler.query_graphs + sampler.corpus_graphs )\n",
    "                    with open(fp, 'wb') as f:\n",
    "                        pickle.dump(RWK, f)\n",
    "    \n",
    "            \n",
    "                all_results[task][dataset] = (_, evaluate_embeddings_similarity_kernels(av, RWK, sampler))    \n",
    "    \n",
    "        else:\n",
    "            for dataset in [\"ptc_fm\",\"ptc_fr\",\"ptc_mm\",\"ptc_mr\",\"mutag\",\"aids\"]:\n",
    "                all_results[task][dataset] =  get_result(task_dict[task][dataset],dataset)\n",
    "                \n",
    "    with open(fp_300, 'wb') as f:\n",
    "        pickle.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This reproduces Table14 in Appendix (run above cell before running this cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[hbt!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c|c|c|c|c|c} \n",
      "\\hline \n",
      " & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\hline\n",
      "GMN-embed & 0.63 $\\pm$ 0.04 & 0.75 $\\pm$ 0.05 & 0.76 $\\pm$ 0.04 & 0.77 $\\pm$ 0.04 & 0.89 $\\pm$ 0.05 & 0.79 $\\pm$ 0.04 \\\\\n",
      "GMN-colbert & 0.76 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 & 0.81 $\\pm$ 0.04 & 0.77 $\\pm$ 0.04 & 0.85 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 \\\\\n",
      "GMN-colbert-mlp & 0.72 $\\pm$ 0.04 & 0.81 $\\pm$ 0.04 & 0.79 $\\pm$ 0.04 & 0.83 $\\pm$ 0.04 & 0.88 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 \\\\\n",
      "NeuroMatch & 0.67 $\\pm$ 0.03 & 0.69 $\\pm$ 0.04 & 0.74 $\\pm$ 0.05 & 0.61 $\\pm$ 0.04 & 0.84 $\\pm$ 0.05 & 0.76 $\\pm$ 0.04 \\\\\n",
      "ISONET & 0.86 $\\pm$ 0.04 & 0.90 $\\pm$ 0.03 & 0.91 $\\pm$ 0.03 & 0.89 $\\pm$ 0.03 & 0.92 $\\pm$ 0.04 & 0.89 $\\pm$ 0.03 \\\\\n",
      "Node-align(Node loss) & 0.83 $\\pm$ 0.03 & 0.86 $\\pm$ 0.03 & 0.86 $\\pm$ 0.03 & 0.84 $\\pm$ 0.03 & 0.89 $\\pm$ 0.04 & 0.85 $\\pm$ 0.04 \\\\\n",
      "Node-align(Edge loss) & 0.80 $\\pm$ 0.05 & 0.85 $\\pm$ 0.03 & 0.83 $\\pm$ 0.04 & 0.81 $\\pm$ 0.04 & 0.82 $\\pm$ 0.06 & 0.87 $\\pm$ 0.04 \\\\\n",
      "GMN-embed-Asym & 0.78 $\\pm$ 0.04 & 0.82 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 & 0.90 $\\pm$ 0.05 & 0.84 $\\pm$ 0.04 \\\\\n",
      "ISONET-Sym & 0.86 $\\pm$ 0.03 & 0.84 $\\pm$ 0.04 & 0.87 $\\pm$ 0.04 & 0.80 $\\pm$ 0.04 & 0.89 $\\pm$ 0.05 & 0.88 $\\pm$ 0.03 \\\\\n",
      "\\end{tabular} \n",
      "\\end{table}\n",
      "\\begin{table}[hbt!]\n",
      "\\centering\n",
      "\\begin{tabular}{l|c|c|c|c|c|c} \n",
      "\\hline \n",
      " & PTC\\_FR & PTC\\_FM & PTC\\_MM & PTC\\_MR  & MUTAG & AIDS \\\\\\hline\n",
      "GMN-embed & 0.95 $\\pm$ 0.03 & 0.84 $\\pm$ 0.07 & 0.90 $\\pm$ 0.05 & 0.93 $\\pm$ 0.04 & 0.95 $\\pm$ 0.04 & 1.00 $\\pm$ 0.00 \\\\\n",
      "GMN-colbert & 0.96 $\\pm$ 0.03 & 0.86 $\\pm$ 0.05 & 0.90 $\\pm$ 0.05 & 0.94 $\\pm$ 0.04 & 0.93 $\\pm$ 0.05 & 0.97 $\\pm$ 0.03 \\\\\n",
      "GMN-colbert-mlp & 0.83 $\\pm$ 0.06 & 0.96 $\\pm$ 0.03 & 0.95 $\\pm$ 0.04 & 0.94 $\\pm$ 0.04 & 0.98 $\\pm$ 0.02 & 0.92 $\\pm$ 0.04 \\\\\n",
      "NeuroMatch & 0.95 $\\pm$ 0.03 & 0.93 $\\pm$ 0.04 & 0.93 $\\pm$ 0.04 & 0.82 $\\pm$ 0.06 & 0.91 $\\pm$ 0.05 & 0.89 $\\pm$ 0.05 \\\\\n",
      "ISONET & 0.96 $\\pm$ 0.03 & 1.00 $\\pm$ 0.00 & 0.98 $\\pm$ 0.02 & 0.97 $\\pm$ 0.03 & 0.94 $\\pm$ 0.04 & 1.00 $\\pm$ 0.00 \\\\\n",
      "Node-align(Node loss) & 0.94 $\\pm$ 0.03 & 0.98 $\\pm$ 0.02 & 0.97 $\\pm$ 0.03 & 1.00 $\\pm$ 0.00 & 0.98 $\\pm$ 0.02 & 0.96 $\\pm$ 0.04 \\\\\n",
      "Node-align(Edge loss) & 0.92 $\\pm$ 0.05 & 0.94 $\\pm$ 0.03 & 0.93 $\\pm$ 0.04 & 0.86 $\\pm$ 0.05 & 0.92 $\\pm$ 0.05 & 0.93 $\\pm$ 0.04 \\\\\n",
      "GMN-embed-Asym & 0.89 $\\pm$ 0.05 & 0.97 $\\pm$ 0.03 & 0.91 $\\pm$ 0.05 & 0.94 $\\pm$ 0.04 & 0.88 $\\pm$ 0.06 & 0.97 $\\pm$ 0.03 \\\\\n",
      "ISONET-Sym & 0.94 $\\pm$ 0.03 & 0.93 $\\pm$ 0.05 & 0.95 $\\pm$ 0.04 & 1.00 $\\pm$ 0.00 & 0.91 $\\pm$ 0.05 & 0.98 $\\pm$ 0.02 \\\\\n",
      "\\end{tabular} \n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "id2 = 1 #metric_id_map = {'MAP':1, 'MRR':4, 'MNDCG':7}\n",
    "print_metric_table(all_results, id2)\n",
    "print_metric_table(all_results, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ed89929ccbe55263ad2347eb76c4f76df95420967d838f6b96d893fe6c17892"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
